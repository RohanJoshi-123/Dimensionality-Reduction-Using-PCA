# Dimensionality-Reduction-Using-PCA

## ğŸ“Œ Project Overview
This project demonstrates Dimensionality Reduction using Principal Component Analysis (PCA). The objective is to reduce high-dimensional data into fewer dimensions while preserving maximum variance and important information.

Dimensionality reduction helps improve:
- Model performance
- Training speed
- Data visualization
- Overfitting reduction

This project was developed as part of my Data Science Internship Program.

---

## ğŸ¯ Objectives
- Understand high-dimensional data
- Implement Principal Component Analysis (PCA)
- Reduce features while retaining maximum variance
- Visualize transformed data
- Compare original vs reduced dataset

---

## ğŸ§  What is PCA?
Principal Component Analysis (PCA) is a statistical technique used to transform correlated features into a smaller number of uncorrelated variables called Principal Components.

Key concepts:
- Variance maximization
- Eigenvalues & Eigenvectors
- Feature transformation
- Explained variance ratio

---

## ğŸ› ï¸ Technologies Used
- Python
- NumPy
- Pandas
- Matplotlib
- Scikit-learn
- Jupyter Notebook

## ğŸ“‚ Project Structure
```
Dimensionality_Reduction_PCA.ipynb
README.md

## âš™ï¸ Implementation Steps
1. Import required libraries
2. Load dataset
3. Preprocess data (scaling if required)
4. Apply PCA
5. Determine explained variance
6. Transform dataset
7. Visualize reduced dimensions

## ğŸ“Š Results
- Successfully reduced high-dimensional data
- Retained maximum variance using principal components
- Improved data visualization in 2D/3D space
- Demonstrated how PCA simplifies complex datasets

## ğŸ“ˆ Applications of PCA
- Image compression
- Face recognition
- Noise reduction
- Data visualization
- Feature extraction in Machine Learning

## ğŸ‘¨â€ğŸ’» Author
Rohan Joshi  
Data Science Intern
